using Game2048: bitboard_to_array, Dirs


struct MDP_mcts
    Î³
    ğ’®
    ğ’œ
    T
    R
    TR
end

struct MonteCarloTreeSearch
    ğ’« # problem
    N # visit counts
    Q # action value estimates
    d # depth
    m # number of simulations
    c # exploration constant
    U # value function estimate
end

function (Ï€::MonteCarloTreeSearch)(s)
    for k in 1:Ï€.m
        simulate!(Ï€, s)
    end
    possible_actions = possible_moves(s)
    dir = argmax(
        Dict(a=>Ï€.Q[(s,a)] for a in possible_actions)
    )
    return dir
end

bonus(Nsa, Ns) = Nsa == 0 ? Inf : sqrt(log(Ns)/Nsa)

function explore(Ï€::MonteCarloTreeSearch, s)
    ğ’œ, N, Q, c = Ï€.ğ’«.ğ’œ, Ï€.N, Ï€.Q, Ï€.c
    possible_actions = possible_moves(s)

    Ns = sum(N[(s,a)] for a in possible_actions)
    Ns = (Ns == 0) ? Inf : Ns
    dir = argmax(
        Dict(a=>Q[(s,a)] + c*sqrt(log(Ns)/N[(s,a)]) for a in possible_actions)
    )
    return dir
end



function simulate!(Ï€::MonteCarloTreeSearch, s, d=Ï€.d)
    if d â‰¤ 0
        return Ï€.U(s)
    end
    ğ’«, N, Q, c = Ï€.ğ’«, Ï€.N, Ï€.Q, Ï€.c
    ğ’œ, TR, Î³ = ğ’«.ğ’œ, ğ’«.TR, ğ’«.Î³
    if !haskey(N, (s, first(ğ’œ)))
        for a in ğ’œ
            N[(s,a)] = 0
            Q[(s,a)] = 0.0
        end
        return Ï€.U(s)
    end
    a = explore(Ï€, s)
    s_prime = TR(s,a) #no reward
    q = Î³*simulate!(Ï€, s_prime, d-1)
    N[(s,a)] += 1
    Q[(s,a)] += (q-Q[(s,a)])/N[(s,a)]
    return q
end